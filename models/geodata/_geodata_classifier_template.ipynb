{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "import descartes\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.layers import Dense  \n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and describing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_repository_path(repository_name):\n",
    "    \"\"\"\n",
    "    Return local absolute path from home directory\n",
    "    to the repository folder (including it).\n",
    "    Arg.: Name of the repository.\n",
    "    \"\"\"\n",
    "    wd_path = os.getcwd()\n",
    "    split_wd_path = wd_path.split(\"/\")\n",
    "    tfm_position = split_wd_path.index(repository_name)\n",
    "    local_path_split = split_wd_path[:tfm_position+1]\n",
    "    return \"/\".join(local_path_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE\n",
    "local_path = get_local_repository_path(\"tfm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUTS\n",
    "save_dir = os.path.join(os.path.abspath(os.getcwd()), \"outputs\", model_name)\n",
    "# Create outputs folder\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DATA SET DIRECTORY\n",
    "source_dir = \"data/geodata/preprocessing/outputs\"\n",
    "# Importing the dataset\n",
    "full_dataset = pd.read_csv(os.path.join(local_path, source_dir, \"filtered_coordinates.csv\"))\n",
    "full_dataset.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = full_dataset.drop(columns=\"coordinate_uncertainty\")\n",
    "dataset.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_species_list(df):\n",
    "    return df[\"species_name\"].tolist()\n",
    "\n",
    "def get_occurrence_count(item_list):\n",
    "    item_count = []\n",
    "    unique_items = unique(item_list)\n",
    "    for item in unique_items:\n",
    "        item_count.append(item_list.count(item))\n",
    "    return item_count\n",
    "\n",
    "def unique(item_list):\n",
    "    unique_list = []\n",
    "    for item in item_list:\n",
    "        if item not in unique_list:\n",
    "            unique_list.append(item)\n",
    "    return unique_list\n",
    "\n",
    "def plot_coordenates_count(occurrence_count, species_name, path_to_plot):\n",
    "    \"\"\"\n",
    "    Plot number of coordenates per class.\n",
    "    \"\"\"\n",
    "    occurrence_count = list(reversed(occurrence_count))\n",
    "    species_name = list(reversed(species_name))\n",
    "    plt.figure(figsize=(10,10)) \n",
    "    plt.barh(species_name, occurrence_count, color=\"#7cd9b7\", height=0.8, edgecolor=\"grey\")\n",
    "    plt.title(\"Number of georeferences per class\")\n",
    "    # x axis, species\n",
    "    xint = range(0,(max(occurrence_count)+1),10)\n",
    "    plt.xticks(xint)\n",
    "    plt.xlabel(\"Number of georeferences\")\n",
    "    # y axis, image count\n",
    "    plt.ylabel(\"Species\", rotation=\"vertical\")\n",
    "    plt.yticks(fontsize=8)\n",
    "    # save file\n",
    "    plt.savefig(path_to_plot)\n",
    "\n",
    "species_name = unique(get_species_list(dataset))\n",
    "occurrence_count = get_occurrence_count(get_species_list(dataset))   \n",
    "plot_coordenates_count(occurrence_count, species_name, os.path.join(save_dir, \"occurrences_per_class.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is class imbalance. I suggest eliminating Pyrus cordata class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def column_to_list(df, column):\n",
    "    \"\"\"\n",
    "    Return a list of elements of column from a data frame.\n",
    "    Args.: a dataframe and a name of a column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return df[column].tolist()\n",
    "    except:\n",
    "        print(f\"Unable to extract {column} from {df}.\")\n",
    "\n",
    "def plot_coordinates_frequency(coordinates_df, destination_path):\n",
    "    \"\"\"\n",
    "    Return an histogram with the frequency of the coordinates.\n",
    "    Arg.: a data frame with latitude and longitude\n",
    "    \"\"\"\n",
    "    latitude = column_to_list(coordinates_df, \"latitude\")\n",
    "    longitude = column_to_list(coordinates_df, \"longitude\")\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(8, 8))\n",
    "    #fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('Coordinates distribution')\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.hist(latitude, bins = 80, alpha = 0.5, \n",
    "             color=\"lightgreen\", \n",
    "             edgecolor=\"grey\")\n",
    "    plt.ylabel(\"Frequency\", fontsize=8)\n",
    "    plt.xlabel(\"Latitude (decimal degrees)\", fontsize=8)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.hist(longitude, bins = 80, alpha = 0.5, \n",
    "            color=\"lightpink\", \n",
    "            edgecolor=\"grey\")\n",
    "    plt.ylabel(\"Frequency\", fontsize=8)\n",
    "    plt.xlabel(\"Longitude (decimal degrees)\", fontsize=8)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend()\n",
    "    plt.savefig(destination_path)\n",
    "    \n",
    "plot_coordinates_frequency(dataset,os.path.join(save_dir, \"histogram.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK map\n",
    "shape_file = os.path.join(local_path,\"data/geodata/uk_maps/GBR_adm0.shp\") # UK map\n",
    "\n",
    "def plot_occurrences_map(coordinates_df, shape_file, destination_path):\n",
    "    \"\"\"\n",
    "    Return an occurrences map in .png format.\n",
    "    Args.: a shape file for a geographical area\n",
    "    and a dataframe with at least two columns:\n",
    "    - latitude\n",
    "    - longitde\n",
    "    and a path where to save the map.    \n",
    "    \"\"\"\n",
    "    # Add a new column to the data frame with a combination of coordinates\n",
    "    coordinate_pair = [Point(xy) for xy in zip(coordinates_df.longitude, coordinates_df.latitude)]\n",
    "    coordinates_df['geometry'] = coordinate_pair\n",
    "    coordinates_df.drop(['latitude','longitude'], axis = 1, inplace=True)\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    occurrences_loc = gpd.GeoDataFrame(coordinates_df, crs=crs, geometry=coordinate_pair)\n",
    "    occurrences_loc.shape\n",
    " \n",
    "    shape_map = gpd.read_file(shape_file)\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    occurrences_loc.geometry.plot(marker=\"d\", color=\"green\", markersize=1, ax=ax, label = \"Species occurrences\")\n",
    "    shape_map.plot(color=\"grey\", ax=ax, alpha = 0.2)\n",
    "   \n",
    "    plt.title('Species distribution map')\n",
    "    plt.legend()\n",
    "    plt.savefig(destination_path)\n",
    "    \n",
    "plot_occurrences_map(dataset, shape_file,os.path.join(save_dir, \"occurrences_map.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS\n",
    "class_names = list(set(column_to_list(dataset, \"species_name\")))\n",
    "print(f\"{len(class_names)} classes in dataset.\")\n",
    "print(f\"Classes names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT FOR EACH MODEL\n",
    "# Model description\n",
    "model_description = f\"\"\"\n",
    "{model_name}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save model description\n",
    "with open(os.path.join(save_dir,\"model_description.txt\"), \"w\") as file:\n",
    "    with redirect_stdout(file):\n",
    "        print(model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the data from the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class names\n",
    "class_names.sort()\n",
    "species_to_number = {species_name:class_names.index(species_name) for species_name in class_names}\n",
    "species_to_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset['target']=dataset.apply(lambda r:species_to_number[r.species_name],axis=1)\n",
    "dataset.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_num = dataset.drop(columns=\"species_name\")\n",
    "dataset_num.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset_num, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors (latitude and longitude)\n",
    "X_train = train.iloc[:, 1:3].values  \n",
    "X_val = val.iloc[:, 1:3].values \n",
    "X_test = test.iloc[:, 1:3].values \n",
    "print(f\"Train predictor shape: {X_train.shape}\")\n",
    "print(f\"Validation predictor shape: {X_val.shape}\")\n",
    "print(f\"Test predictor shape: {X_test.shape}\")\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train = train.iloc[:, 2].values  \n",
    "y_val = val.iloc[:, 2].values \n",
    "test_labels = test.iloc[:, 2].values \n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(test_labels)\n",
    "print(f\"Train target shape: {y_train.shape}\")\n",
    "print(f\"Validation target shape: {y_val.shape}\")\n",
    "print(f\"Test target shape: {y_test.shape}\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_not_norm = dataset.drop(columns=\"target\")\n",
    "dataset_not_norm.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of features normalization\n",
    "x = dataset_not_norm.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "dataset_norm = pd.DataFrame(x_scaled)\n",
    "dataset_norm.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = normalize(X_train)\n",
    "X_val_norm = normalize(X_val)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model network architecture\n",
    "\n",
    "The simplest network architecture constists of 3 layers:\n",
    "\n",
    "- Input layer, with a number of nodes equal to the number of features in the model.\n",
    "- Hidden layer, with a variable number of nodes. \n",
    "- Output layer, with a number of nodes equal to the number of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The hidden layers\n",
    "\n",
    "The hidden layers can have different characteristics depending of their use. \n",
    "The transformations applied by the convolutional layers have shown the best results for image classification, so they will be the main component in this model.\n",
    "\n",
    "\n",
    "##### The fully connected layer\n",
    "\n",
    "A fully connected layer is an all purpose layer where each node receive the inputs from all the nodes from the previous layer, multiplied by their weights, sumed and transformed by the activation funcion.\n",
    "\n",
    "In keras, the fully connected layer is called `Dense`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and training the model with `Keras`\n",
    "\n",
    "In `Keras` the model is defined with the `Sequential` method as a linear stack ot layers. The **input layer** is implicit in the first layer (a network with 3 layers will have 2 in `keras Sequential` method).\n",
    "\n",
    "The **input shape** is into the first layer. The model inputs are the tensors or arrays. Images have 3 dimensions: **width**, **height** and **channels**. The width and the height are measured in pixels and the channels reference the color values (the channel value is 1 if it is in black and white and 3 if it is color in RGB (Red, Green, blue) or HSV (hue, saturation, value) formats - 2 and 4 are black and white or color with an alpha channel (transparency). \n",
    "\n",
    "The **activation function** that has to be specified in each layer transforms the input data so that the output doen't have a linear relation with the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2H_LR001_500_50PT\" # Number of hidden layers, learning rate, epochs, checkout patience.\n",
    "model = Sequential()\n",
    "tf.keras.layers.Flatten(input_shape=(X_train_norm.shape[-1],))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(class_names), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(optimizer = \"adam\", loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping (when loss does not fall anymore to avoid overfitting)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience = 50)\n",
    "\n",
    "# Checkpoint to save model weights and history before it stops training\n",
    "checkpoint_filepath = os.path.join(save_dir, \"/tmp/checkpoint\")\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                                              save_weights_only = True,\n",
    "                                                              monitor= \"val_acc\",\n",
    "                                                              save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = model.fit(\n",
    "        X_train_norm,\n",
    "        y_train,\n",
    "        validation_data=(X_val_norm, y_val),\n",
    "        batch_size = 10, \n",
    "        epochs = epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model history to csv\n",
    "history_df = pd.DataFrame(history.history) \n",
    "history_df.to_csv(os.path.join(save_dir, \"model_history.csv\"), sep=\",\", index=False)\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(os.path.join(save_dir, \"weights.h5\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters measured during model training\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acc = history_dict[\"acc\"]\n",
    "    val_acc = history_dict[\"val_acc\"]\n",
    "    loss = history_dict[\"loss\"]\n",
    "    val_loss = history_dict[\"val_loss\"]\n",
    "except:\n",
    "    try:\n",
    "        acc = history_dict[\"accuracy\"]\n",
    "        val_acc = history_dict[\"val_accuracy\"]\n",
    "        loss = history_dict[\"loss\"]\n",
    "        val_loss = history_dict[\"val_loss\"]\n",
    "    except:\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_acc_loss(acc,val_acc,loss,val_loss,epochs):\n",
    "    epochs_range = range(epochs)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.suptitle(model_name)\n",
    "    # Accuracy plots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    # Loss plots\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label=\"Training Loss\") \n",
    "    plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir,\"acc_loss_plot.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(acc,val_acc,loss,val_loss,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "\n",
    "When the model predicts significantly better the training set than the validation set, it is a sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuse model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model again, it has to be built and then the weights added:\n",
    "\n",
    "```\n",
    "# load pre-trained model with the weights\n",
    "vgg16_model = tf.keras.applications.VGG16()\n",
    "# Add the layers of vgg16 model to a new sequential model \n",
    "model = Sequential()\n",
    "for layer in vgg16_model.layers[:-1]: # remove last layer\n",
    "    model.add(layer)\n",
    "# Freeze the weights in the layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "# Add last layer for categories\n",
    "model.add(Dense(len(class_names), activation = \"softmax\"))  \n",
    "\n",
    "cnn.load_weights(os.path.join(save_dir, \"model.h5\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get back the accuracy and loss data:\n",
    "\n",
    "- Open the CSV with the model history.\n",
    "- Save it to a dictionary.\n",
    "\n",
    "```\n",
    "history_df = pd.read_csv(os.path.join(save_dir, \"model_history.csv\"))\n",
    "history_dict = history_df.to_dict()\n",
    "try: # the key names vary across tf versions\n",
    "    acc = np.array(list(history_dict[\"acc\"].values()))\n",
    "    val_acc = np.array(list(history_dict[\"val_acc\"].values()))\n",
    "    loss = np.array(list(history_dict[\"loss\"].values()))\n",
    "    val_loss = np.array(list(history_dict[\"val_loss\"].values()))\n",
    "    epochs_range = np.array(range(epochs))\n",
    "except:\n",
    "    try:\n",
    "        acc = np.array(list(history_dict[\"accuracy\"].values()))\n",
    "        val_acc = np.array(list(history_dict[\"val_accuracy\"].values()))\n",
    "        loss = np.array(list(history_dict[\"loss\"].values()))\n",
    "        val_loss = np.array(list(history_dict[\"val_loss\"].values()))\n",
    "        epochs_range = np.array(range(epochs))\n",
    "    except:\n",
    "        pass\n",
    "plot_acc_loss(acc,val_acc,loss,val_loss,epochs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_labels_to_index(test_labels, class_names):\n",
    "    \"\"\"\n",
    "    Return a 1D array of integers with the corresponding\n",
    "    number for a class.\n",
    "    Args.: - A list with the class name of each item in \n",
    "          the test data set.\n",
    "           - A sorted list with the possible class names. \n",
    "    Eg.: test_labels[1] = \"Buxus_sempervirens\" corresponds to index 4\n",
    "         in the list of class names.\n",
    "    \"\"\"\n",
    "    test_labels_index = []\n",
    "    for i in range(len(test_labels)):\n",
    "        ind = class_names.index(test_labels[i])\n",
    "        test_labels_index.append(ind)\n",
    "    return np.array(test_labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(test_dir)\n",
    "test_labels = get_test_labels(test_files)\n",
    "test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_index = test_labels_to_index(test_labels, class_names)\n",
    "test_labels_index[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the probability of classifiying each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability of predicting each class for each image\n",
    "predictions = model.predict(X_test, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions is a 2D array with a shape: (number of examples in test, number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted class for each example\n",
    "def predicted_class(predictions):\n",
    "    \"\"\"\n",
    "    Return a 1D array with the predicted class for each example.\n",
    "    Arg.: 2D array predictions of shape (number of examples, number of classes)\n",
    "    \"\"\"\n",
    "    pred_class = []\n",
    "    for i in range(len(predictions)):\n",
    "        higher_prob = max(predictions[i])\n",
    "        ind, = np.where(np.isclose(predictions[i], higher_prob))\n",
    "        pred_class.append(ind[0])\n",
    "    return np.array(pred_class)\n",
    "\n",
    "pred_class = predicted_class(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confussion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the confusion matrix\n",
    "cm = tf.math.confusion_matrix(test_labels, pred_class) \n",
    "# Convert from tensor to array\n",
    "sess = tf.Session()\n",
    "conf_mat = sess.run(cm)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.BuGn)\n",
    "    plt.title(\"Confusion matrix - \"+ model_name, fontsize = 22)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=90)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.xlabel(\"Predicted\", fontsize = 18)\n",
    "    plt.ylabel(\"Actual\", fontsize = 18)\n",
    "    plt.savefig(os.path.join(save_dir,\"conf_matrix.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, np.array(class_names), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
