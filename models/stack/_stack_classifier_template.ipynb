{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack image and geodata models (50:50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "import descartes\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.utils import plot_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_repository_path(repository_name):\n",
    "    \"\"\"\n",
    "    Return local absolute path from home directory\n",
    "    to the repository folder (including it).\n",
    "    Arg.: Name of the repository.\n",
    "    \"\"\"\n",
    "    wd_path = os.getcwd()\n",
    "    split_wd_path = wd_path.split(\"/\")\n",
    "    tfm_position = split_wd_path.index(repository_name)\n",
    "    local_path_split = split_wd_path[:tfm_position+1]\n",
    "    return \"/\".join(local_path_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE\n",
    "local_path = get_local_repository_path(\"tfm\")\n",
    "model_name = \"stack_example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUTS\n",
    "save_dir = os.path.join(os.path.abspath(os.getcwd()), \"outputs\", model_name)\n",
    "# Create outputs folder\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS (for the weights)\n",
    "input_dir = os.path.join(os.path.abspath(os.getcwd()), \"inputs\")\n",
    "geo_weights = \"geo_weights_1D_1000.h5\"\n",
    "img_weights = \"img_weights_VGG16_b1b2b3PT_500_30P.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GEODATA SET DIRECTORY\n",
    "geo_source_dir = \"data/geodata/preprocessing/outputs\"\n",
    "# Importing the dataset\n",
    "full_dataset = pd.read_csv(os.path.join(local_path, geo_source_dir, \"filtered_coordinates.csv\"))\n",
    "dataset = full_dataset.drop(columns=\"coordinate_uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SET DIRECTORIES\n",
    "img_source_dir = \"data/images/image_preprocessing/processed_images_train_val_test/\"\n",
    "train_dir = os.path.join(local_path, img_source_dir, \"train\")\n",
    "val_dir = os.path.join(local_path, img_source_dir, \"val\")\n",
    "test_dir = os.path.join(local_path, img_source_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS\n",
    "img_class_names = sorted(os.listdir(train_dir))\n",
    "print(f\"{len(img_class_names)} classes in dataset.\")\n",
    "print(f\"Classes names: {img_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS\n",
    "geo_class_names = list(set(full_dataset[\"species_name\"].tolist()))\n",
    "print(f\"{len(geo_class_names)} classes in dataset.\")\n",
    "print(f\"Classes names: {geo_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT FOR EACH MODEL\n",
    "# Model description\n",
    "model_description = f\"\"\"\n",
    "{model_name}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save model description\n",
    "with open(os.path.join(save_dir,\"model_description.txt\"), \"w\") as file:\n",
    "    with redirect_stdout(file):\n",
    "        print(model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEODATA SAMPLE\n",
    "full_dataset = pd.read_csv(os.path.join(local_path, geo_source_dir, \"filtered_coordinates.csv\"))\n",
    "dataset = full_dataset.drop(columns=\"coordinate_uncertainty\")\n",
    "\n",
    "# Encode class names\n",
    "geo_class_names.sort()\n",
    "species_to_number = {species_name:geo_class_names.index(species_name) for species_name in geo_class_names}\n",
    "dataset['target']=dataset.apply(lambda r:species_to_number[r.species_name],axis=1)\n",
    "dataset.sample(n=5)\n",
    "dataset_num = dataset.drop(columns=\"species_name\")\n",
    "dataset_num.sample(n=5)\n",
    "\n",
    "# Split in train, val and test subsets\n",
    "train, test = train_test_split(dataset_num, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "# Extract predictors (latitude and longitude)\n",
    "X_train = train.iloc[:, 1:3].values  \n",
    "X_val = val.iloc[:, 1:3].values \n",
    "X_test = test.iloc[:, 1:3].values \n",
    "print(f\"Train predictor shape: {X_train.shape}\")\n",
    "print(f\"Validation predictor shape: {X_val.shape}\")\n",
    "print(f\"Test predictor shape: {X_test.shape}\")\n",
    "print(X_train)\n",
    "\n",
    "# Encode labels\n",
    "y_train = train.iloc[:, 2].values  \n",
    "y_val = val.iloc[:, 2].values \n",
    "test_labels = test.iloc[:, 2].values \n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(test_labels)\n",
    "print(f\"Train target shape: {y_train.shape}\")\n",
    "print(f\"Validation target shape: {y_val.shape}\")\n",
    "print(f\"Test target shape: {y_test.shape}\")\n",
    "print(y_train)\n",
    "\n",
    "# Normalize train  val predictors\n",
    "X_train_norm = normalize(X_train)\n",
    "X_val_norm = normalize(X_val)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION ImageDataGenerator \n",
    "img_height = 224 \n",
    "img_width = 224\n",
    "color_mode= \"rgb\"\n",
    "class_mode=\"categorical\"                                  \n",
    "shuffle=True                                                               \n",
    "seed = 1234 \n",
    "def plot_images(images_arr):\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(15,15))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "train_datagen_no_aug = ImageDataGenerator(rescale=1./255)  \n",
    "train_array_no_aug = train_datagen_no_aug.flow_from_directory(directory = train_dir,\n",
    "                                            target_size=(img_width, img_height)\n",
    "                                            ) \n",
    "sample_training_images, _ = next(train_array_no_aug)\n",
    "plot_images(sample_training_images[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEODATA\n",
    "# Architecture\n",
    "geo_model = Sequential()\n",
    "geo_model.add(layers.Dense(64, input_dim=2, activation='relu'))\n",
    "geo_model.add(layers.Dense(len(class_names), activation='softmax'))\n",
    "# Load weights\n",
    "#geo_model.load_weights(os.path.join(input_dir, geo_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGES\n",
    "# Architecture\n",
    "loaded_model = tf.keras.applications.VGG16()\n",
    "img_model = Sequential()\n",
    "for layer in loaded_model.layers[:-1]: \n",
    "    img_model.add(layer)\n",
    "img_model.add(Dense(len(class_names), activation = \"softmax\"))\n",
    "# Load weights\n",
    "img_model.load_weights(os.path.join(input_dir, img_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "img_input = keras.Input(shape=(224,224,3))\n",
    "geo_input = keras.Input(shape=(2))\n",
    "\n",
    "y1 = img_model(img_input)\n",
    "y2 = geo_model(geo_input)\n",
    "output = layers.average([y1,y2])\n",
    "ensemble_model = keras.Model(inputs=[img_input, geo_input], outputs = output)\n",
    "keras.utils.plot_model(ensemble_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.compile(optimizer = \"adam\", loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "history = model.fit(\n",
    "    \"image\": train_array,\n",
    "    \"geodata\": X_train_norm,\n",
    "        y_train,\n",
    "        validation_data=(X_val_norm, y_val),\n",
    "        batch_size = 10, \n",
    "        epochs = epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters measured during model training\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    acc = history_dict[\"acc\"]\n",
    "    val_acc = history_dict[\"val_acc\"]\n",
    "    loss = history_dict[\"loss\"]\n",
    "    val_loss = history_dict[\"val_loss\"]\n",
    "except:\n",
    "    try:\n",
    "        acc = history_dict[\"accuracy\"]\n",
    "        val_acc = history_dict[\"val_accuracy\"]\n",
    "        loss = history_dict[\"loss\"]\n",
    "        val_loss = history_dict[\"val_loss\"]\n",
    "    except:\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_acc_loss(acc,val_acc,loss,val_loss,epochs):\n",
    "    epochs_range = range(epochs)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.suptitle(model_name)\n",
    "    # Accuracy plots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label=\"Training Accuracy\")\n",
    "    plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    # Loss plots\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label=\"Training Loss\") \n",
    "    plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.savefig(os.path.join(save_dir,\"acc_loss_plot.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "plot_acc_loss(acc,val_acc,loss,val_loss,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "\n",
    "When the model predicts significantly better the training set than the validation set, it is a sign of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuse model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the model again, it has to be built and then the weights added:\n",
    "\n",
    "```\n",
    "# load pre-trained model with the weights\n",
    "vgg16_model = tf.keras.applications.VGG16()\n",
    "# Add the layers of vgg16 model to a new sequential model \n",
    "model = Sequential()\n",
    "for layer in vgg16_model.layers[:-1]: # remove last layer\n",
    "    model.add(layer)\n",
    "# Freeze the weights in the layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "# Add last layer for categories\n",
    "model.add(Dense(len(class_names), activation = \"softmax\"))  \n",
    "\n",
    "cnn.load_weights(os.path.join(save_dir, \"model.h5\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get back the accuracy and loss data:\n",
    "\n",
    "- Open the CSV with the model history.\n",
    "- Save it to a dictionary.\n",
    "\n",
    "```\n",
    "history_df = pd.read_csv(os.path.join(save_dir, \"model_history.csv\"))\n",
    "history_dict = history_df.to_dict()\n",
    "try: # the key names vary across tf versions\n",
    "    acc = np.array(list(history_dict[\"acc\"].values()))\n",
    "    val_acc = np.array(list(history_dict[\"val_acc\"].values()))\n",
    "    loss = np.array(list(history_dict[\"loss\"].values()))\n",
    "    val_loss = np.array(list(history_dict[\"val_loss\"].values()))\n",
    "    epochs_range = np.array(range(epochs))\n",
    "except:\n",
    "    try:\n",
    "        acc = np.array(list(history_dict[\"accuracy\"].values()))\n",
    "        val_acc = np.array(list(history_dict[\"val_accuracy\"].values()))\n",
    "        loss = np.array(list(history_dict[\"loss\"].values()))\n",
    "        val_loss = np.array(list(history_dict[\"val_loss\"].values()))\n",
    "        epochs_range = np.array(range(epochs))\n",
    "    except:\n",
    "        pass\n",
    "plot_acc_loss(acc,val_acc,loss,val_loss,epochs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_labels_to_index(test_labels, class_names):\n",
    "    \"\"\"\n",
    "    Return a 1D array of integers with the corresponding\n",
    "    number for a class.\n",
    "    Args.: - A list with the class name of each item in \n",
    "          the test data set.\n",
    "           - A sorted list with the possible class names. \n",
    "    Eg.: test_labels[1] = \"Buxus_sempervirens\" corresponds to index 4\n",
    "         in the list of class names.\n",
    "    \"\"\"\n",
    "    test_labels_index = []\n",
    "    for i in range(len(test_labels)):\n",
    "        ind = class_names.index(test_labels[i])\n",
    "        test_labels_index.append(ind)\n",
    "    return np.array(test_labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = os.listdir(test_dir)\n",
    "test_labels = get_test_labels(test_files)\n",
    "test_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_index = test_labels_to_index(test_labels, class_names)\n",
    "test_labels_index[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the probability of classifiying each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the probability of predicting each class for each image\n",
    "predictions = model.predict_generator(test_array,steps=1,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions is a 2D array with a shape: (number of examples in test, number of classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted class for each example\n",
    "def predicted_class(predictions):\n",
    "    \"\"\"\n",
    "    Return a 1D array with the predicted class for each example.\n",
    "    Arg.: 2D array predictions of shape (number of examples, number of classes)\n",
    "    \"\"\"\n",
    "    pred_class = []\n",
    "    for i in range(len(predictions)):\n",
    "        higher_prob = max(predictions[i])\n",
    "        ind, = np.where(np.isclose(predictions[i], higher_prob))\n",
    "        pred_class.append(ind[0])\n",
    "    return np.array(pred_class)\n",
    "\n",
    "pred_class = predicted_class(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confussion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the confusion matrix\n",
    "cm = tf.math.confusion_matrix(test_labels_index, pred_class) \n",
    "# Convert from tensor to array\n",
    "sess = tf.Session()\n",
    "conf_mat = sess.run(cm)\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.BuGn)\n",
    "    plt.title(\"Confusion matrix - \"+ model_name, fontsize = 22)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=90)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    plt.savefig(os.path.join(save_dir,\"conf_matrix.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_mat, np.array(class_names), model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
